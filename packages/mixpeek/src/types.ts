/**
 * Types for @remotion/mixpeek integration
 *
 * These types support Mixpeek's multimodal extractor which processes
 * video, audio, image, text, and GIF content using:
 * - Whisper for transcription
 * - E5-Large for text embeddings (1024D)
 * - Vertex AI for multimodal embeddings (1408D)
 * - Gemini for visual descriptions
 * - OCR for text extraction
 */

import type {Caption} from '@remotion/captions';

/**
 * Configuration options for the Mixpeek client
 */
export interface MixpeekConfig {
		/**
		 * Your Mixpeek API key
		 * Get one at https://dash.mixpeek.com
		 */
	apiKey: string;
		/**
		 * Optional namespace for multi-tenant isolation
		 */
	namespace?: string;
		/**
		 * Optional base URL for custom API endpoints
		 */
	baseUrl?: string;
		/**
		 * Optional request timeout in milliseconds
		 * @default 30000
		 */
	timeout?: number;
}

/**
 * A segment from Mixpeek's video analysis (multimodal extractor output)
 */
export interface MixpeekVideoSegment {
		/**
		 * Start time of the segment in seconds
		 */
	startTime: number;
		/**
		 * End time of the segment in seconds
		 */
	endTime: number;
		/**
		 * Transcribed text for this segment (Whisper transcription)
		 */
	text?: string;
		/**
		 * Scene description generated by Gemini visual analysis
		 */
	sceneDescription?: string;
		/**
		 * OCR-extracted text from the video frames
		 */
	ocrText?: string;
		/**
		 * URL to the thumbnail image for this segment
		 */
	thumbnailUrl?: string;
		/**
		 * URL to the video segment clip
		 */
	videoSegmentUrl?: string;
		/**
		 * Detected objects in this segment
		 */
	detectedObjects?: string[];
		/**
		 * Detected faces/entities
		 */
	entities?: MixpeekEntity[];
		/**
		 * Confidence score (0-1)
		 */
	confidence?: number;
		/**
		 * Legacy embedding field (for backward compatibility)
		 */
	embedding?: number[];
		/**
		 * Vertex AI multimodal embedding (1408 dimensions)
		 * Field: multimodal_extractor_v1_multimodal_embedding
		 */
	multimodalEmbedding?: number[];
		/**
		 * E5-Large transcription embedding (1024 dimensions)
		 * Field: multimodal_extractor_v1_transcription_embedding
		 */
	transcriptionEmbedding?: number[];
		/**
		 * Additional metadata
		 */
	metadata?: Record<string, unknown>;
}

/**
 * An entity detected by Mixpeek
 */
export interface MixpeekEntity {
		/**
		 * Entity type (e.g., 'person', 'speaker', 'object')
		 */
	type: string;
		/**
		 * Entity label or name
		 */
	label: string;
		/**
		 * Bounding box coordinates [x, y, width, height] as percentages
		 */
	boundingBox?: [number, number, number, number];
		/**
		 * Confidence score (0-1)
		 */
	confidence?: number;
}

/**
 * Video analysis result from Mixpeek
 */
export interface MixpeekVideoAnalysis {
		/**
		 * Unique identifier for this analysis
		 */
	id: string;
		/**
		 * Original video URL or path
		 */
	sourceUrl: string;
		/**
		 * Duration of the video in seconds
		 */
	duration: number;
		/**
		 * Frames per second of the video
		 */
	fps?: number;
		/**
		 * All analyzed segments
		 */
	segments: MixpeekVideoSegment[];
		/**
		 * Full transcript if available
		 */
	transcript?: string;
		/**
		 * Overall video summary
		 */
	summary?: string;
		/**
		 * Tags/labels for the entire video
		 */
	tags?: string[];
		/**
		 * When the analysis was created
		 */
	createdAt: string;
}

/**
 * Search result from Mixpeek
 */
export interface MixpeekSearchResult {
		/**
		 * Unique identifier
		 */
	id: string;
		/**
		 * Relevance score (0-1)
		 */
	score: number;
		/**
		 * The matching segment
		 */
	segment: MixpeekVideoSegment;
		/**
		 * Source document/video information
		 */
	source: {
			/**
			 * Video/document ID
			 */
			id: string;
			/**
			/**
	 * Types for @remotion/mixpeek integration
	 *
	 * These types support Mixpeek's multimodal extractor which processes
	 * video, audio, image, text, and GIF content using:
	 * - Whisper for transcription
	 * - E5-Large for text embeddings (1024D)
	 * - Vertex AI for multimodal embeddings (1408D)
	 * - Gemini for visual descriptions
	 * - OCR for t
